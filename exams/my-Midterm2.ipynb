{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559895d2",
   "metadata": {},
   "source": [
    "# Midterm 2\n",
    "\n",
    "## FINM 36700 - 2024\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cde8d3",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc273c1a",
   "metadata": {},
   "source": [
    "## Please note the following:\n",
    "\n",
    "Points\n",
    "* The exam is 100 points.\n",
    "* You have 120 minutes to complete the exam.\n",
    "* For every minute late you submit the exam, you will lose one point.\n",
    "\n",
    "\n",
    "Submission\n",
    "* You will upload your solution to the `Midterm 2` assignment on Canvas, where you downloaded this. \n",
    "* Be sure to **submit** on Canvas, not just **save** on Canvas.\n",
    "* Your submission should be readable, (the graders can understand your answers.)\n",
    "* Your submission should **include all code used in your analysis in a file format that the code can be executed.** \n",
    "\n",
    "Rules\n",
    "* The exam is open-material, closed-communication.\n",
    "* You do not need to cite material from the course github repo - you are welcome to use the code posted there without citation.\n",
    "\n",
    "Advice\n",
    "* If you find any question to be unclear, state your interpretation and proceed. We will only answer questions of interpretation if there is a typo, error, etc.\n",
    "* The exam will be graded for partial credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f27b1",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**All data files are found in the class github repo, in the `data` folder.**\n",
    "\n",
    "This exam makes use of the following data files:\n",
    "* `midterm_2_data.xlsx`\n",
    "\n",
    "This file contains the following sheets:\n",
    "- for Section 2:\n",
    "    * `sector stocks excess returns` - MONTHLY excess returns for 49 sector stocks\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5\n",
    "- for Section 3:\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6e066",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "| Problem | Points |\n",
    "|---------|--------|\n",
    "| 1       | 25     |\n",
    "| 2       | 40     |\n",
    "| 3       | 35     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2fc26",
   "metadata": {},
   "source": [
    "### Each numbered question is worth 5 points unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156e8f",
   "metadata": {},
   "source": [
    "# 1. Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4bc8",
   "metadata": {},
   "source": [
    "#### No Data Needed\n",
    "\n",
    "These problems do not require any data file. Rather, analyze them conceptually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2ec27",
   "metadata": {},
   "source": [
    "### 1.1.\n",
    "\n",
    "Historically, which pricing factor among the ones we studied has shown a considerable decrease in importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100c6f8-8df4-45a3-a28a-12f3fc4ab783",
   "metadata": {},
   "source": [
    "Historically, within the Fama-French Five-Factor (FF5F) model, the Value factor (HML) has shown a considerable decrease in importance. The HML factor measures the return difference between stocks with high and low book-to-market ratios. Over recent decades, the value premium—the excess returns of value stocks over growth stocks—has significantly diminished and has even turned negative at times as noticed in the casestudy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c8109",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.2.\n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the cross-sectional MAE. \n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the time-series MAE. \n",
    "\n",
    "Along with stating T/F, explain your reasoning for the two statements."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccd9f532-77f5-4754-abbb-e6ee43179f94",
   "metadata": {},
   "source": [
    "1. False. Adding an additional factor to a factor model doesn't necessarily decrease the cross-sectional Mean Absolute Error (MAE). The new factor might not have any explanatory power for the test assets. If the added factor's coefficient turns out to be zero or statistically insignificant, it won't improve the model's fit. There's also the risk of overfitting: while the in-sample MAE might decrease, this doesn't guarantee a decrease across all cross-sections, especially out-of-sample. If the new factor doesn't provide new information beyond what's already captured by the existing factors, the MAE might stay the same.\n",
    "\n",
    "2. False. Similarly, adding another factor doesn't necessarily decrease the time-series MAE. The new factor may not be relevant to the time-series variation of individual asset returns. If there's no correlation between the factor and the asset's returns over time, it won't improve the MAE. Introducing more factors also increases the complexity of the model and can introduce estimation errors, which might not reduce the MAE. If the time-series regression estimates the coefficient of the new factor as zero or statistically insignificant, the MAE will remain unchanged.\n",
    "\n",
    "In both cases, adding an additional factor doesn't guarantee a decrease in MAE; the error might stay the same or even increase due to estimation variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c00026",
   "metadata": {},
   "source": [
    "### 1.3.\n",
    "\n",
    "Consider the scenario in which you are helping two people with investments.\n",
    "\n",
    "* The young person has a 50 year investment horizon.\n",
    "* The elderly person has a 10 year investment horizon.\n",
    "* Both individuals have the same portfolio holdings.\n",
    "\n",
    "State who has the more certain cumulative return and explain your reasoning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "90c22951-6f07-421c-85a4-3665e18c8848",
   "metadata": {},
   "source": [
    "Between the two individuals, the elderly person with a 10-year investment horizon has the more certain cumulative return. Even though both have the same portfolio holdings, the shorter investment period means there's less time for market volatility to impact the overall return. Over a longer horizon like 50 years, the young person faces greater uncertainty due to the compounding effects of market fluctuations and the unpredictability of long-term economic conditions. Therefore, the elderly person's cumulative return is more certain because the shorter time frame reduces the potential for significant deviations in returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d4d72",
   "metadata": {},
   "source": [
    "### 1.4.\n",
    "\n",
    "Suppose we find that the 10-year bond yield works well as a new pricing factor, along with `MKT`.\n",
    "\n",
    "Consider two ways of building this new factor.\n",
    "1. Directly use the index of 10-year yields, `YLD`\n",
    "1. Construct a Fama-French style portfolio of equities, `FFYLD`. (Rank all the stocks by their correlation to bond yield changes, and go long the highest ranked and shor tthe lowest ranked.)\n",
    "\n",
    "Could you test the model with `YLD` and the model with `FFYLD` in the exact same ways? Explain."
   ]
  },
  {
   "cell_type": "raw",
   "id": "05649b33-14df-44bb-a20d-4802eddc041c",
   "metadata": {},
   "source": [
    "No, we couldn't test the model with YLD and the model with FFYLD in exactly the same ways. The main reason is that YLD is a non-traded factor as it represents the index of 10-year bond yields—while FFYLD is a traded factor because it's constructed as a portfolio of equities in a Fama-French style. Since YLD isn't a traded asset, we can't directly use the same regression techniques we would with a portfolio like FFYLD.\n",
    "\n",
    "When dealing with traded factors like FFYLD, we can apply standard asset pricing tests, like time-series regressions and cross-sectional regressions, to estimate factor risk premiums and assess the model's performance. These methods rely on the factor being a return series from an actual investment strategy.\n",
    "\n",
    "In contrast, with a non-traded factor like YLD, we have to adjust our approach. We might need to create a \"mimicking portfolio\" that captures the movements of the bond yields using traded assets, or use alternative econometric techniques designed for non-traded variables. The testing procedures differ because the nature of the factors is different—one is directly investable, and the other isn't.\n",
    "\n",
    "So, because YLD and FFYLD are fundamentally different in terms of being non-traded and traded factors, respectively, we can't test the models incorporating them in the exact same ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d238",
   "metadata": {},
   "source": [
    "### 1.5.\n",
    "\n",
    "Suppose we implement a momentum strategy on cryptocurrencies rather than US stocks.\n",
    "\n",
    "Conceptually speaking, but specific to the context of our course discussion, how would the risk profile differ from the momentum strategy of US equities?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab59f0d7-6420-49ec-9f6f-eb2c11a45291",
   "metadata": {},
   "source": [
    "Using crypto ETFs for a momentum strategy introduces both similarities and differences compared to US equities. Crypto ETFs add regulation and transparency, which helps mitigate some risks like liquidity and custody issues, making the investment process more like traditional equities.\n",
    "\n",
    "However, even with these improvements, the risk profile is still quite different. Cryptocurrencies are more volatile and are influenced by factors such as technological developments, specific regulatory changes, and speculative market sentiment. These can lead to rapid price swings that aren't as common in the stock market.\n",
    "\n",
    "While momentum factors might still apply—assets that have performed well may continue to do so—their effectiveness can vary in the crypto space due to its relative youth, lower liquidity, and different investor behavior. So, despite increased regulation and transparency through crypto ETFs, the same factors don't necessarily matter in the same way, and the risk profile remains distinct from that of US equities.\n",
    "\n",
    "In short there's other unexplanable idiosyncracies involved with crypto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ce7d4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8a354",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Pricing and Tangency Portfolio\n",
    "\n",
    "You work in a hedge fund that believes that the AQR 4-Factor Model (present in Homework 5) is the perfect pricing model for stocks.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\tilde{r}^i \\right] = \\beta^{i,\\text{MKT}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{MKT}} \\right] + \\beta^{i,\\text{HML}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{HML}} \\right] + \\beta^{i,\\text{RMW}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{RMW}} \\right] + \\beta^{i,\\text{UMD}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{UMD}} \\right]\n",
    "$$\n",
    "\n",
    "The factors are available in the sheet `factors excess returns`.\n",
    "\n",
    "The hedge fund invests in sector-tracking ETFs available in the sheet `sectors excess returns`. You are to allocate into these sectors according to a mean-variance optimization with...\n",
    "\n",
    "* regularization: elements outside the diagonal covariance matrix divided by 2.\n",
    "* modeled risk premia: expected excess returns given by the factor model rather than just using the historic sample averages.\n",
    "\n",
    "You are to train the portfolio and test out-of-sample. The timeframes should be:\n",
    "* Training timeframe: Jan-2018 to Dec-2022.\n",
    "* Testing timeframe: Jan-2023 to most recent observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334d4395-4362-4666-bd75-226b5ddae471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "FILEIN = '../data/midterm_2_data.xlsx'\n",
    "\n",
    "sector_excess_returns = pd.read_excel(FILEIN, sheet_name='sector excess returns', index_col=0)\n",
    "factors_excess_returns = pd.read_excel(FILEIN, sheet_name='factors excess returns', index_col=0)\n",
    "training_data_sectors = sector_excess_returns.loc['2018-01':'2022-12']\n",
    "training_data_factors = factors_excess_returns.loc['2018-01':'2022-12']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db465bc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.1.\n",
    "(8pts)\n",
    "\n",
    "Calculate the model-implied expected excess returns of every asset.\n",
    "\n",
    "The time-series estimations should...\n",
    "* NOT include an intercept. (You assume the model holds perfectly.)\n",
    "* use data from the `training` timeframe.\n",
    "\n",
    "With the time-series estimates, use the `training` timeframe's sample average of the factors as the factor premia. Together, this will give you the model-implied risk premia, which we label as\n",
    "$$\n",
    "\\lambda_i := \\mathbb{E}[\\tilde{r}_i]\n",
    "$$\n",
    "\n",
    "* Store $\\lambda_i$ and $\\boldsymbol{\\beta}^i$ for each asset.\n",
    "* Print $\\lambda_i$ for `Agric`, `Food`, `Soda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5b575a-6f0c-4eb2-90da-ec4064c260d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agri: 0.003655102106916501\n",
      "Food: 0.005454267868380435\n",
      "Soda: 0.007336244651963082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "factor_premia = training_data_factors.mean()\n",
    "betas = pd.DataFrame(index=training_data_sectors.columns, columns=factor_premia.index)\n",
    "\n",
    "for sector in training_data_sectors.columns:\n",
    "    X = training_data_factors\n",
    "    Y = training_data_sectors[sector]\n",
    "    betas.loc[sector] = np.linalg.lstsq(X, Y, rcond=None)[0]\n",
    "\n",
    "lambda_i = betas.dot(factor_premia)\n",
    "lambda_agric = lambda_i['Agric']\n",
    "lambda_food = lambda_i['Food ']\n",
    "lambda_soda = lambda_i['Soda ']\n",
    "print(\"Agri:\", lambda_agric)\n",
    "print(\"Food:\", lambda_food)\n",
    "print(\"Soda:\", lambda_soda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f19a6c-ced4-4152-9840-86c7d2cb489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tangency_weights(\n",
    "    returns: pd.DataFrame,\n",
    "    cov_mat: str = 1,\n",
    "    return_graphic: bool = False,\n",
    "    return_port_ret: bool = False,\n",
    "    target_ret_rescale_weights: Union[None, float] = None,\n",
    "    annual_factor: int = 12,\n",
    "    name: str = 'Tangency'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates tangency portfolio weights based on the covariance matrix of returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): Time series of returns.\n",
    "    cov_mat (str, default=1): Covariance matrix for calculating tangency weights.\n",
    "    return_graphic (bool, default=False): If True, plots the tangency weights.\n",
    "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
    "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    name (str, default='Tangency'): Name for labeling the weights and portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: Tangency portfolio weights or portfolio returns if `return_port_ret` is True.\n",
    "    \"\"\"\n",
    "    returns = returns.copy()\n",
    "    \n",
    "    if 'date' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "    returns.index.name = 'date'\n",
    "\n",
    "    if cov_mat == 1:\n",
    "        cov_inv = np.linalg.inv((returns.cov() * annual_factor))\n",
    "    else:\n",
    "        cov = returns.cov()\n",
    "        covmat_diag = np.diag(np.diag((cov)))\n",
    "        covmat = cov_mat * cov + (1 - cov_mat) * covmat_diag\n",
    "        cov_inv = np.linalg.pinv((covmat * annual_factor))  \n",
    "        \n",
    "    ones = np.ones(returns.columns.shape) \n",
    "    mu = returns.mean() * annual_factor\n",
    "    scaling = 1 / (np.transpose(ones) @ cov_inv @ mu)\n",
    "    tangent_return = scaling * (cov_inv @ mu)\n",
    "    tangency_wts = pd.DataFrame(\n",
    "        index=returns.columns,\n",
    "        data=tangent_return,\n",
    "        columns=[f'{name} Weights']\n",
    "    )\n",
    "    port_returns = returns @ tangency_wts.rename({f'{name} Weights': f'{name} Portfolio'}, axis=1)\n",
    "\n",
    "    if return_graphic:\n",
    "        tangency_wts.plot(kind='bar', title=f'{name} Weights')\n",
    "\n",
    "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
    "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
    "        tangency_wts[[f'{name} Weights']] *= scaler\n",
    "        port_returns *= scaler\n",
    "        tangency_wts = tangency_wts.rename(\n",
    "            {f'{name} Weights': f'{name} Weights Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
    "            axis=1\n",
    "        )\n",
    "        port_returns = port_returns.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    if cov_mat != 1:\n",
    "        port_returns = port_returns.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
    "        tangency_wts = tangency_wts.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
    "        \n",
    "    if return_port_ret:\n",
    "        return port_returns\n",
    "    return tangency_wts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80c6b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.2.\n",
    "\n",
    "Use the expected excess returns derived from (2.1) with the **regularized** covariance matrix to calculate the weights of the tangency portfolio.\n",
    "\n",
    "- Use the covariance matrix only for `training` timeframe.\n",
    "- Calculate and store the vector of weights for all the assets.\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t} = \\dfrac{\\tilde{\\Sigma}^{-1} \\bm{\\lambda}}{\\bm{1}' \\tilde{\\Sigma}^{-1} \\bm{\\lambda}}\n",
    "$$\n",
    "\n",
    "Where $\\tilde{\\Sigma}^{-1}$ is the regularized covariance-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c68597-b4e5-4b5a-9efc-ee295f9df087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tangency Regularized 0.50 Weights    0.14409\n",
       " Name: Agric, dtype: float64,\n",
       " Tangency Regularized 0.50 Weights   -0.06981\n",
       " Name: Food , dtype: float64,\n",
       " Tangency Regularized 0.50 Weights    0.32268\n",
       " Name: Soda , dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tangency_weights = calc_tangency_weights(\n",
    "    returns=training_data_sectors,\n",
    "    cov_mat=0.5,\n",
    "    return_port_ret=False,\n",
    "    annual_factor=12,\n",
    "    name='Tangency'\n",
    ")\n",
    "\n",
    "tangency_agric = tangency_weights.loc['Agric']\n",
    "tangency_food = tangency_weights.loc['Food ']\n",
    "tangency_soda = tangency_weights.loc['Soda ']\n",
    "\n",
    "tangency_agric, tangency_food, tangency_soda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c171c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.3.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b96d44d6-9f6a-49ad-9e61-97296959537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_mean_vol_sharpe(data,portfolio = None,portfolio_name = 'Portfolio',annualize = 12):\n",
    "    \n",
    "    if portfolio is None:\n",
    "        returns = data\n",
    "    else:\n",
    "        returns = data @ portfolio\n",
    "    \n",
    "    output = returns.agg(['mean','std'])\n",
    "    output.loc['sharpe'] = output.loc['mean'] / output.loc['std']\n",
    "    \n",
    "    output.loc['mean'] *= annualize\n",
    "    output.loc['std'] *= np.sqrt(annualize)\n",
    "    output.loc['sharpe'] *= np.sqrt(annualize)\n",
    "    \n",
    "    if portfolio is None:\n",
    "        pass\n",
    "    else:\n",
    "        output.columns = [portfolio_name]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0acad9c8-1067-4d1c-a302-ea2a690d4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_sectors = sector_excess_returns.loc['2023-01':]\n",
    "\n",
    "common_sectors = tangency_weights.index.intersection(testing_data_sectors.columns)\n",
    "\n",
    "tangency_weights_aligned = tangency_weights.loc[common_sectors]\n",
    "\n",
    "testing_data_sectors_aligned = testing_data_sectors[tangency_weights.index]\n",
    "\n",
    "performance_stats = stats_mean_vol_sharpe(\n",
    "    data=testing_data_sectors_aligned,\n",
    "    portfolio=tangency_weights,\n",
    "    portfolio_name='Tangency Portfolio',\n",
    "    annualize=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4cf0fb9-d3f9-494e-a3a0-1268bb669f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tangency Portfolio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.176801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.153010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpe</th>\n",
       "      <td>1.155487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tangency Portfolio\n",
       "mean              0.176801\n",
       "std               0.153010\n",
       "sharpe            1.155487"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6f8bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.4.\n",
    "\n",
    "(7pts)\n",
    "\n",
    "Construct the same tangency portfolio as in `2.2` but with one change:\n",
    "* replace the risk premia of the assets, (denoted $\\lambda_i$) with the sample averages of the excess returns from the `training` set.\n",
    "\n",
    "So instead of using $\\lambda_i$ suggested by the factor model (as in `2.1-2.3`) you're using sample averages for $\\lambda_i$.\n",
    "\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b9ecca-96e8-47f9-bd7d-439fde1a2024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tangency Regularized 0.50 Sample Averages Weights    0.14409\n",
       " Name: Agric, dtype: float64,\n",
       " Tangency Regularized 0.50 Sample Averages Weights   -0.06981\n",
       " Name: Food , dtype: float64,\n",
       " Tangency Regularized 0.50 Sample Averages Weights    0.32268\n",
       " Name: Soda , dtype: float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lambda_i_adjusted = training_data_sectors.mean() * 12  \n",
    "\n",
    "\n",
    "tangency_weights_with_sample_averages = calc_tangency_weights(\n",
    "    returns=training_data_sectors,\n",
    "    cov_mat=0.5,\n",
    "    return_port_ret=False,\n",
    "    annual_factor=12,\n",
    "    name='Tangency Sample Averages'\n",
    ")\n",
    "\n",
    "\n",
    "common_sectors_sample_averages = tangency_weights_with_sample_averages.index.intersection(testing_data_sectors.columns)\n",
    "testing_data_sectors_aligned_sample_averages = testing_data_sectors[common_sectors_sample_averages]\n",
    "tangency_weights_with_sample_averages_aligned = tangency_weights_with_sample_averages.loc[common_sectors_sample_averages]\n",
    "\n",
    "\n",
    "tangency_agric_sample_avg = tangency_weights_with_sample_averages_aligned.loc['Agric']\n",
    "tangency_food_sample_avg = tangency_weights_with_sample_averages_aligned.loc['Food ']\n",
    "tangency_soda_sample_avg = tangency_weights_with_sample_averages_aligned.loc['Soda ']\n",
    "\n",
    "\n",
    "performance_with_sample_averages = stats_mean_vol_sharpe(\n",
    "    data=testing_data_sectors_aligned_sample_averages,\n",
    "    portfolio=tangency_weights_with_sample_averages_aligned,\n",
    "    portfolio_name='Tangency Portfolio Sample Averages',\n",
    "    annualize=12\n",
    ")\n",
    "\n",
    "\n",
    "tangency_agric_sample_avg, tangency_food_sample_avg, tangency_soda_sample_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172cbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.5.\n",
    "\n",
    "Which allocation performed better in the `testing` period: the allocation based on premia from the factor model or from the sample averages?\n",
    "\n",
    "Why might this be?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "058547ca-c6a0-4a5a-9aa8-9d9cb4245a8a",
   "metadata": {},
   "source": [
    "\n",
    "neither performed better; both showed the same results.\n",
    "\n",
    "this might be because:\n",
    "data similarity: factor model risk premia and sample averages may have been close during training, leading to similar weights.\n",
    "market stability: in stable markets, both methods can produce similar outcomes.\n",
    "market efficiency: efficient markets may cause factor exposures and sample averages to align, especially if the factors are widely recognized.\n",
    "in more volatile conditions, factor models might perform differently, as they capture systematic risk factors beyond historical averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a442fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.6.\n",
    "Suppose you now want to build a tangency portfolio solely from the factors, without using the sector ETFs.\n",
    "\n",
    "- Calculate the weights of the tangency portfolio using `training` data for the factors.\n",
    "- Again, regularize the covariance matrix of factor returns by dividing off-diagonal elements by 2.\n",
    "\n",
    "Report, in the `testing` period, the factor-based tangency stats **annualized**...\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e3ffe1c-e743-4954-a215-3673dacf5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt prompt : fix my code:\n",
    "\n",
    "# factor_cov_matrix = training_data_factors.cov()\n",
    "# regularized_cov_matrix = factor_cov_matrix.copy()\n",
    "# off_diagonal_mask = ~np.eye(factor_cov_matrix.shape[1], dtype=int)  \n",
    "# regularized_cov_matrix[off_diagonal_mask] /= 2 \n",
    "\n",
    "# factor_mean_returns = training_data_factors.median() * 10  \n",
    "\n",
    "# tangency_weights_factors_only = calc_tangency_weights(\n",
    "#     returns=training_data_factors,\n",
    "#     cov_mat=factor_cov_matrix, \n",
    "#     return_port_ret=True,       \n",
    "#     annual_factor=10,          \n",
    "#     name='Tangency Factors'\n",
    "# )\n",
    "\n",
    "# testing_data_factors_aligned = testing_data_factors[~tangency_weights_factors_only.index]  # Added `~` to invert the index alignment\n",
    "# performance_factors_only = stats_mean_vol_sharpe(\n",
    "#     data=testing_data_factors_aligned,\n",
    "#     portfolio=tangency_weights_factors_only,\n",
    "#     portfolio_name='Tangency Portfolio', \n",
    "#     annualize=False  \n",
    "# )\n",
    "\n",
    "# performance_factors_only * 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "563b2823-9aab-452e-8e71-864a95bd2ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tangency Portfolio Factors Only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.062376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.058191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpe</th>\n",
       "      <td>1.071918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tangency Portfolio Factors Only\n",
       "mean                           0.062376\n",
       "std                            0.058191\n",
       "sharpe                         1.071918"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "factor_cov_matrix = training_data_factors.cov()\n",
    "regularized_cov_matrix = factor_cov_matrix.copy()\n",
    "off_diagonal_mask = ~np.eye(factor_cov_matrix.shape[0], dtype=bool)\n",
    "regularized_cov_matrix.values[off_diagonal_mask] /= 2\n",
    "\n",
    "factor_mean_returns = training_data_factors.mean() * 12  \n",
    "\n",
    "tangency_weights_factors_only = calc_tangency_weights(\n",
    "    returns=training_data_factors,\n",
    "    cov_mat=0.5,                   \n",
    "    return_port_ret=False,          \n",
    "    annual_factor=12,\n",
    "    name='Tangency Factors Only'\n",
    ")\n",
    "\n",
    "\n",
    "testing_data_factors_aligned = testing_data_factors[tangency_weights_factors_only.index]  # Align columns for testing period\n",
    "performance_factors_only = stats_mean_vol_sharpe(\n",
    "    data=testing_data_factors_aligned,\n",
    "    portfolio=tangency_weights_factors_only,\n",
    "    portfolio_name='Tangency Portfolio Factors Only',\n",
    "    annualize=12\n",
    ")\n",
    "\n",
    "performance_factors_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6cc36",
   "metadata": {},
   "source": [
    "### 2.7.\n",
    "\n",
    "Based on the hedge fund's beliefs, would you prefer to use the ETF-based tangency or the factor-based tangency portfolio? Explain your reasoning. Note that you should answer based on broad principles and not on the particular estimation results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "de5b598e-d772-4f17-876b-c275c9d1097a",
   "metadata": {},
   "source": [
    "based on the hedge fund's beliefs, it would make more sense to use the factor-based tangency portfolio. since the fund views the factor model as the perfect pricing model for stocks, building a portfolio directly from the factors aligns more closely with this belief. factor-based portfolios allow us to directly capture systematic risks (like market, size, value) rather than relying on sector etfs, which may include idiosyncratic risks unrelated to the factors. in theory, a factor-based approach should offer a cleaner, more efficient exposure to expected risk premia, aligning better with the fund's view of systematic factor drivers of returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8eda25",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff849e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 3. Long-Run Returns\n",
    "\n",
    "For this question, use only the sheet `factors excess returns`.\n",
    "\n",
    "Suppose we want to measure the long run returns of various pricing factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be343b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.1.\n",
    "\n",
    "Turn the data into log returns.\n",
    "- Display the first 5 rows of the data.\n",
    "\n",
    "Using these log returns, report the **annualized**\n",
    "* mean\n",
    "* vol\n",
    "* Sharpe\n",
    "\n",
    "### 3.2.\n",
    "\n",
    "Consider 15-year cumulative log excess returns. Following the assumptions and modeling of Lecture 6, report the following 15-year stats:\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n",
    "\n",
    "How do they compare to the estimated stats (1-year horizon) in `3.1`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "953bec72-62ad-4a4e-a438-eb225b5c2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                MKT     HML     RMW     UMD\n",
      "date                                      \n",
      "1980-01-01  0.0551  0.0175 -0.0170  0.0755\n",
      "1980-02-01 -0.0122  0.0061  0.0004  0.0788\n",
      "1980-03-01 -0.1290 -0.0101  0.0146 -0.0955\n",
      "1980-04-01  0.0397  0.0106 -0.0210 -0.0043\n",
      "1980-05-01  0.0526  0.0038  0.0034 -0.0112\n",
      "...            ...     ...     ...     ...\n",
      "2024-04-01 -0.0467 -0.0052  0.0148 -0.0042\n",
      "2024-05-01  0.0434 -0.0166  0.0298 -0.0002\n",
      "2024-06-01  0.0277 -0.0331  0.0051  0.0090\n",
      "2024-07-01  0.0124  0.0573  0.0022 -0.0242\n",
      "2024-08-01  0.0161 -0.0112  0.0085  0.0478\n",
      "\n",
      "[536 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(factors_excess_returns.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34b4f46b-a272-4966-ad30-4a72ea180742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "1980-01-01    0.053636\n",
      "1980-02-01   -0.012275\n",
      "1980-03-01   -0.138113\n",
      "1980-04-01    0.038932\n",
      "1980-05-01    0.051263\n",
      "Name: log_returns, dtype: float64\n",
      "Mean: 1.544518959824679\n",
      "Volatility: 0.7279005281156687\n",
      "Sharpe Ratio: 2.121881905791451\n"
     ]
    }
   ],
   "source": [
    "df = factors_excess_returns\n",
    "df['log_returns'] = np.log(1 + df['MKT'])\n",
    "\n",
    "print(df['log_returns'].head())\n",
    "\n",
    "trading_days = 252\n",
    "annualized_mean = df['log_returns'].mean() * trading_days\n",
    "annualized_vol = df['log_returns'].std() * np.sqrt(trading_days)\n",
    "annualized_sharpe = annualized_mean / annualized_vol\n",
    "\n",
    "print(f\"Mean: {annualized_mean}\")\n",
    "print(f\"Volatility: {annualized_vol}\")\n",
    "print(f\"Sharpe Ratio: {annualized_sharpe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181ba0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.3.\n",
    "\n",
    "What is the probability that momentum factor has a negative mean excess return over the next \n",
    "* single period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662a283-d678-42dd-a322-88b42d522859",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_returns = df['UMD']\n",
    "\n",
    "mean_return = momentum_returns.mean()\n",
    "std_dev = momentum_returns.std()\n",
    "\n",
    "single_period_prob = norm.cdf(0, loc=mean_return, scale=std_dev)\n",
    "\n",
    "mean_15_year = mean_return * 15\n",
    "std_dev_15_year = std_dev * np.sqrt(15)\n",
    "multi_period_prob = norm.cdf(0, loc=mean_15_year, scale=std_dev_15_year)\n",
    "\n",
    "print(f\"single period probability of negative Return: {single_period_prob}\")\n",
    "print(f\"15-year probability of negative mean Return: {multi_period_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137b86c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.4.\n",
    "\n",
    "Recall from the case that momentum has been underperforming since 2009. \n",
    "\n",
    "Using data from 2009 to present, what is the probability that momentum *outperforms* the market factor over the next\n",
    "* period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9819ecc6-23ba-40fe-8f63-9530d674c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of outperformance over a single period: 0.54\n",
      "probability of outperformance over 15 years: 0.65\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "spread = df['MKT'] - df['UMD'] \n",
    "\n",
    "\n",
    "mu, sigma = spread.mean() * 12, spread.std() * np.sqrt(12)\n",
    "\n",
    "\n",
    "single_period_prob = 1 - norm.cdf(0, mu, sigma)\n",
    "\n",
    "multi_period_prob = 1 - norm.cdf(0, mu, sigma / np.sqrt(15))\n",
    "\n",
    "print(\"probability of outperformance over a single period:\", single_period_prob.round(2))\n",
    "print(\"probability of outperformance over 15 years:\", multi_period_prob.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678bc07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.5.\n",
    "Conceptually, why is there such a discrepancy between this probability for 1 period vs. 15 years?\n",
    "\n",
    "What assumption about the log-returns are we making when we use this technique to estimate underperformance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99587170-6db0-40ea-8d6f-746f886c7e7e",
   "metadata": {},
   "source": [
    "the discrepancy between the probability for a single period versus a 15-year period comes down to the effect of compounding and the assumption of normal distribution. when we calculate the probability over a longer period, we're assuming that the returns will average out over time, with the mean return becoming more stable due to the law of large numbers. over 15 years, the cumulative return variance grows more slowly (as the square root of time), leading to a narrower distribution relative to the expected mean. this makes extreme deviations, like underperformance, less likely in the long run compared to a single period.\n",
    "\n",
    "by using this technique, we’re assuming that the log-returns are independent and identically distributed (iid) and follow a normal distribution. this iid assumption implies that each period’s return is unrelated to others, and normality assumes that returns don't have fat tails or skewness, which may not always hold in financial markets, especially over longer horizons where volatility clustering or mean reversion could play a role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f33b7",
   "metadata": {},
   "source": [
    "### 3.6.\n",
    "\n",
    "Using your previous answers, explain what is meant by time diversification."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ab2076c-235f-4c4d-a3fe-72514780fb79",
   "metadata": {},
   "source": [
    "time diversification refers to the idea that the risk of underperformance decreases as the investment horizon lengthens. as we saw with the probability calculations, while there might be a significant chance that momentum underperforms the market over a single period, the probability of underperformance over 15 years is much lower. this is because, over time, the cumulative effect of returns tends to stabilize around the expected mean, and the variance grows at a slower rate (proportional to the square root of time) rather than linearly. essentially, the more periods we invest across, the more likely it is that short-term fluctuations will average out, leading to a narrower distribution of potential outcomes and reducing the chance of extreme negative returns. this concept relies on the assumption that returns are independent and identically distributed, which may not always hold true in real-world markets, but it provides a rationale for why long-term investing can reduce the impact of volatility on overall returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5080207",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.7.\n",
    "\n",
    "Is the probability that `HML` and `UMD` both have negative cumulative returns over the next year higher or lower than the probability that `HML` and `MKT` both have negative cumulative returns over the next year?\n",
    "\n",
    "Answer conceptually, but specifically. (No need to calculate the specific probabilities.)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ad4a718-e6cb-497f-9d95-ff4fa7d2d7c3",
   "metadata": {},
   "source": [
    "the probability that hml and umd both have negative cumulative returns over the next year is likely higher than the probability that hml and mkt both have negative cumulative returns over the same period. this is because hml (value factor) and umd (momentum factor) are typically less correlated with each other than they are with the overall market (mkt). hml and umd represent different investing styles (value vs. momentum), and while they can be negatively correlated or experience independent periods of underperformance, they aren't directly tied to market movements in the same way that hml and mkt are.\n",
    "\n",
    "since mkt represents the overall market return, hml is more likely to move in sync with mkt than with umd, as market downturns often affect value-oriented stocks alongside the broader market. therefore, hml and mkt both experiencing negative returns simultaneously is less probable than hml and umd both doing so, as hml and mkt are more correlated in response to macroeconomic conditions and broad market trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf51ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
